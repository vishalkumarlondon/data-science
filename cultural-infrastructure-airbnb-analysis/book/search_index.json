[
["index.html", "Measuring the impact of cultural infrastructure on Airbnb listings in London Chapter 1 Abstract", " Measuring the impact of cultural infrastructure on Airbnb listings in London Vishal Kumar 2020-01-12 Chapter 1 Abstract This study uses cultural data science to measure the impact of cultural infrastructure on the supply and demand of Airbnb listings in London. Put the rest of the abstract here. Code by Andy, Adam, Fundementals of Data Visualization, have been very helpful and have been referenced thoroughly throughout. This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["load-data.html", "Chapter 3 Load Data 3.1 Load packages 3.2 Download data 3.3 Join the data 3.4 Clean variables before analysis", " Chapter 3 Load Data 3.1 Load packages #----load all the libraries needed library(sf) library(plyr) library(pins) library(tidyverse) library(tidyr) library(sp) #library(spdep) options(scipen = 999) 3.2 Download data 1. Download Airbnb data I collected every Airbnb “listings.csv” file for London from the website Inside Airbnb (http://insideairbnb.com/get-the-data.html) for the years 2017 and 2018, concatinated them all together and put it here (github link) on this project’s GitHub repo. In total, there are 242,490 unqiue listings with 14 columns of attributes. None of that raw data has had any post processing. #----read in Airbnb London listings data from the project GitHub repo as a dataframe airbnb &lt;- read_csv(&quot;data/airbnb-london-2017-2018.csv&quot;) print(dim(airbnb)) ## [1] 242490 14 This study does not look at the temporal change in price - i.e. the price difference between 2017 to 2018 - as the dependent variable. Rather, it averages out the price for each unique listing (by id) between 2017 and 2018. By doing this, adjustments for natural fluctations - for instance, seasonality - are averaged out. Future research may study dive deeper into the differences in price across time and season. #----create a new dataframe by grouping the listings id and creating an average price column price_average &lt;- airbnb %&gt;% group_by(id) %&gt;% summarise(price = mean(price)) colnames(price_average) &lt;- c(&quot;id&quot;, &quot;price_average&quot;) #----join the new dataframe to the original Airbnb data to add a column for the average price per listing between 2017 and 2018 airbnb &lt;- inner_join(airbnb, price_average, by = c(&#39;id&#39;)) #----keep the original Airbnb data my making a `airbnb_old` dataframe airbnb_old &lt;- airbnb #----remove unnecessary columns and remove duplicate listing ids to leave unique listings with average price between 2017 and 2018 #airbnb &lt;- airbnb[, ! colnames(airbnb) %in% c(&quot;price&quot;, &quot;last_review&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;)] airbnb &lt;- airbnb[, ! colnames(airbnb) %in% c(&quot;price&quot;)] airbnb &lt;- airbnb[!duplicated(airbnb[c(&quot;id&quot;)]),] #----turn Airbnb datafram into an sf airbnb &lt;- st_as_sf(airbnb, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) airbnb &lt;- st_transform(airbnb, 27700) At this stage, outliers in price are removed, this is because the average price for some listings went as high as £10,000. All listings that had an average price 2.58 times the mean were removed. Moreover, prices recorded as zero were also removed. #----calculate the mean, min and max and std of the Airbnb prices for the whole of London print(summary(airbnb$price_average)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 45.0 80.0 100.8 125.0 9999.0 airbnb_price_mean &lt;- mean(airbnb$price_average) airbnb_price_std &lt;- sd(airbnb$price_average) airbnb_price_mad &lt;- mad(airbnb$price_average) #----then remove Airbnb where the price is two standard deviations away from the mean - i.e. outliers airbnb &lt;- subset(airbnb, price_average &lt; (airbnb_price_mean+(2.58*airbnb_price_std)) &amp; price_average &gt; (airbnb_price_mean-(2.58*airbnb_price_std))) #----then remove Airbnb where the price is zero airbnb &lt;- airbnb[airbnb$price_average != 0,] print(summary(airbnb$price_average)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 45.00 79.00 93.08 120.00 397.00 #----calculate the mean, min and max and std of the Airbnb prices for the whole of London print(summary(airbnb$price_average)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 45.00 79.00 93.08 120.00 397.00 airbnb_price_mean &lt;- mean(airbnb$price_average) airbnb_price_std &lt;- sd(airbnb$price_average) airbnb_price_mad &lt;- mad(airbnb$price_average) #----then remove Airbnb where the price is two standard deviations away from the mean - i.e. outliers airbnb &lt;- subset(airbnb, price_average &lt; (airbnb_price_mean+(2.58*airbnb_price_std)) &amp; price_average &gt; (airbnb_price_mean-(2.58*airbnb_price_std))) #----then remove Airbnb where the price is zero airbnb &lt;- airbnb[airbnb$price_average != 0,] print(summary(airbnb$price_average)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 44.50 75.00 86.82 120.00 258.50 2. Download Cultural Infrastructure data I collected data for every cultural venue in London by borough from data.london.gov (https://data.london.gov.uk/dataset/cultural-infrastructure-map). Specifically, the “Cultural venues by London borough” zip file was downloaded and then all the files were concatinated. In total there were 10,003 cultural venues in London. This entire dataset was then passed to the Google Places API to calculate the average user rating and total number of reviews for each venue. The total run time to recieve a ratings and reviews took 90 minutes, which is why it was much more efficient to calculated these metrics outside of this R script. For more information on the Google Place API, please visit this link (X) 1,958 venues out of the 10,003 total did not possess any, or had zero, user ratings and reviews on Google Places and were therefore dropped leaving 8,045 venues remaining for the analysis. #----read in the Cultural Infrastructure data from porject GitHubb culture &lt;- read.csv(&quot;data/all-cultural-infra-map-google-places.csv&quot;) print(dim(culture)) ## [1] 10003 25 print(colSums(is.na(culture))) ## BOROUGH Cultural.Venue.Type site_name address1 ## 0 0 0 0 ## address2 address3 borough_code borough_name ## 0 0 0 0 ## latitude longitude easting northing ## 0 0 0 0 ## os_addressbase_uprn ward_2018_code ward_2018_name website ## 4273 0 0 0 ## gss_code runtime API_response formatted_address ## 0 0 0 0 ## name place_id rating types ## 0 0 1556 0 ## user_ratings_total ## 1556 print(dim(culture[culture$rating == 0, ])) ## [1] 1958 25 #----only keep rows from Cultural Infrastructure if the longitude cell is filled in (i.e. not Null) culture &lt;- culture[complete.cases(culture$longitude), ] #----only keep rows from Cultural Infrastructure if the rating cell is filled in (i.e. not Null) culture &lt;- culture[!is.na(as.numeric(as.character(culture$rating))),] #----only keep rows from Cultural Infrastructure if the rating cell is not 0 culture &lt;- culture[culture$rating != 0, ] #----turn the Cultural Infrastructure dataframe into an sf object culture &lt;- st_as_sf(culture, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) culture &lt;- st_transform(culture, 27700) #culture_withPubs &lt;- culture #culture &lt;- culture[!culture$Cultural.Venue.Type == &#39;Pubs&#39;,] 3. Download London shapefile data The GIS shapefile boundaries for London were downloaded from data.london (https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london). This research is interested in studying the Lower Super Output Area (LSOA) 2011 boundary area, however, Ward and Borough level shape files were also stored in a list if necessary. In total there are 4,835 LSOA areas. #----the following code has been adapted from (MacLachlan &amp; Dennett, 2019: Section 10.4.1) #----use the pin function from the pins package to store the GIS London boundary .zip files from data.london pin_london_GIS &lt;- pin(&quot;https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip&quot;) #----grab the shape files for Borough, Ward and LSOA based on their string values and cache s &lt;- grepl(&quot;Borough|Ward_|LSOA_2011&quot;, pin_london_GIS) &amp; grepl(&quot;.shp$&quot;, pin_london_GIS) #----create a list for Borough, Ward and LSOA shape files BoroughsWardsLSOA &lt;- pin_london_GIS[s] #----turn each element in the list into a SF file using the st_read function BoroughsWardsLSOAsf &lt;- lapply(BoroughsWardsLSOA, st_read) ## Reading layer `London_Borough_Excluding_MHW&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/statistical_gis_boundaries_london/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 33 features and 7 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs ## Reading layer `London_Ward_CityMerged&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/statistical_gis_boundaries_london/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 625 features and 7 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs ## Reading layer `LSOA_2011_London_gen_MHW&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/statistical_gis_boundaries_london/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 4835 features and 14 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs BoroughsWardsLSOAsf &lt;- lapply(BoroughsWardsLSOAsf, crs=27700, st_transform) #----create a variable for LSOAs in London by selecting the third element in the list londonLSOA &lt;- BoroughsWardsLSOAsf[[3]] londonWard &lt;- BoroughsWardsLSOAsf[[2]] londonBorough &lt;- BoroughsWardsLSOAsf[[1]] 4. Download the LSOA profile data The LSOA atlas (https://data.london.gov.uk/dataset/lsoa-atlas) provides a summary of demographic and related data for each Lower Super Output Area in Greater London. Some of this attribbute data for each LSOA will be useful as independent variable later in the analysis. This data is downloaded from data.london and subset by 15 columns which are most relevant as per Dudas et al’s (2017) paper. #----run the code below if you want to read in LSOA attribute data using the current 2011 boundaries #----NOTE: There is comparatively less data for the new boundaries compared with the old boundaries londonLSOAProfiles &lt;- read_csv(&quot;https://data.london.gov.uk/download/lsoa-atlas/0193f884-2ccd-49c2-968e-28aa3b1c480d/lsoa-data.csv&quot;, na = c(&quot;&quot;, &quot;NA&quot;, &quot;n/a&quot;), locale = locale(encoding = &#39;Latin1&#39;), col_names = TRUE) ## Warning: 2 parsing failures. ## row col expected actual file ## 1348 House Prices;Median Price (£);2014 a double . &#39;https://data.london.gov.uk/download/lsoa-atlas/0193f884-2ccd-49c2-968e-28aa3b1c480d/lsoa-data.csv&#39; ## 2873 House Prices;Median Price (£);2014 a double . &#39;https://data.london.gov.uk/download/lsoa-atlas/0193f884-2ccd-49c2-968e-28aa3b1c480d/lsoa-data.csv&#39; select.me &lt;- c(&#39;Lower Super Output Area&#39;, &#39;Population Density;Area (Hectares);&#39;, &#39;Population Density;Persons per hectare;2013&#39;, &#39;Ethnic Group;BAME (%);2011&#39;, &#39;Country of Birth;% Not United Kingdom;2011&#39;, &#39;Tenure;Owned outright (%);2011&#39;, &#39;Tenure;Owned with a mortgage or loan (%);2011&#39;, &#39;House Prices;Median Price (£);2014&#39;, &#39;Economic Activity;Employment Rate;2011&#39;, &#39;Qualifications;% Highest level of qualification: Level 4 qualifications and above;2011&#39;, &#39;Household Income, 2011/12;Median Annual Household Income estimate (£)&#39;, &#39;Public Transport Accessibility Levels (2014);% 4-6 (good access)&#39;, &#39;2013 Census Population;Age Structure;16-29&#39;, &#39;2014 Census Population;Age Structure;30-44&#39;, &#39;Dwelling type;All Households;2011&#39;) londonLSOAProfiles &lt;- londonLSOAProfiles[,select.me] 5. Download Inner/Outer London boundaries It will be useful to compare Airbnb lisitings and cultural infrastructure in Inner London vs Outer London as it is likely that the freqncy, price and average ratings of the former will all be higher than the later. Download the Inner/Outer London boundaries from data.london #----use the pin function from the pins package to store the Inner and Outer London boundary .zip files from data.london pin_inner_outer &lt;- pin(&quot;https://data.london.gov.uk/download/inner-and-outer-london-boundaries-london-plan-consultation-2009/684e59f2-9208-4da1-bf67-d8dfeb72c047/lp-consultation-oct-2009-inner-outer-london-shp.zip&quot;) #----pull out the Inner and Outer London shapefile i_o &lt;- grepl(&quot;lp-consultation-oct-2009-inner-outer&quot;, pin_inner_outer) &amp; grepl(&quot;.shp$&quot;, pin_inner_outer) #----create a list for the shape file inner_outer &lt;- pin_inner_outer[i_o] #----turn the shapefile into a SF file using the st_read function inner_outerSF &lt;- st_read(inner_outer) ## Reading layer `lp-consultation-oct-2009-inner-outer-london&#39; from data source `/Users/vishalkumar.london/Library/Caches/pins/local/lp_consultation_oct_2009_inner_outer_london_shp/lp-consultation-oct-2009-inner-outer-london.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 2 features and 5 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs st_transform(inner_outerSF, 27700) ## Simple feature collection with 2 features and 5 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## epsg (SRID): 27700 ## proj4string: +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs ## Boundary Source Area_Ha Shape_Leng Shape_Area ## 1 Inner London London Plan Consultation Draft 34863.3 117020.7 348632957 ## 2 Outer London London Plan Consultation Draft 124606.8 373367.7 1246068121 ## geometry ## 1 POLYGON ((522055.6 178014.7... ## 2 POLYGON ((503611.2 175520.4... 3.3 Join the data 1. Join Airbnb data with LSOA shapefile The functions below join the Airbnb data to the LSOA areas in London and a) count the number of Airbnb listings per LSOA, b) calculating the average price of Airbnb lisitngs per LSOA, c) counting the number of Airbnb user reviews per LSOA, and d) calculating the average number of Airbnb user reviews per LSOA. JoinAirbnb_count &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----count the number of points per LSOA count &lt;- as.data.frame(plyr::count(joined$LSOA11CD)) names(count) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_freq&quot;) return(count) } JoinAirbnb_price &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average price of Airbnb per LSOA price &lt;- aggregate(price_average~LSOA11CD, joined, mean) names(price) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_price&quot;) return(price) } JoinAirbnb_NOreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average price of Airbnb per LSOA reviews &lt;- aggregate(number_of_reviews~LSOA11CD, joined, sum) names(reviews) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_no_reviews&quot;) return(reviews) } JoinAirbnb_AVreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average price of Airbnb per LSOA reviews &lt;- aggregate(number_of_reviews~LSOA11CD, joined, mean) names(reviews) &lt;- c(&quot;LSOA11CD&quot;, &quot;airbnb_av_reviews&quot;) return(reviews) } #----//////////////////////////////////////////////////////////////////////// #----AIRBNB COUNT----# #----use the first function to count the number of Airbnbs in each LSOA airbnbLSOA_count &lt;- JoinAirbnb_count(airbnb, londonLSOA) #----AIRBNB PRICE----# #----use the second function to calculate the average price of Airbnb listings in each LSOA airbnbLSOA_price &lt;- JoinAirbnb_price(airbnb, londonLSOA) #----AIRBNB NUMBER OF REVIEWS----# #----use the third function to count the number of Airbnb reviews in each LSOA airbnbLSOA_review_count &lt;- JoinAirbnb_NOreviews(airbnb, londonLSOA) #----AIRBNB AVERAGE NUMBER OF REVIEWS----# #----use the fourth function to calculate the average number of Airbnb reviews in each LSOA airbnbLSOA_review_average &lt;- JoinAirbnb_AVreviews(airbnb, londonLSOA) 2. Join Culture data with LSOA shapefile The functions below join the Cultural Infrastructure data to the LSOA areas in London and a) count the number of Cultural Infrastructure by cultural venue type per LSOA, b) calculating the average Google Places user rating by cultural venue type per LSOA, c) calculating the average number of Google Places user reviews by cultural venue type per LSOA, and d) counting the total number of Google Places user reviews by cultural venue type per LSOA. JoinCulture_count &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----count the number cultural venue types per LSOA #source----https://stackoverflow.com/questions/10879551/frequency-count-of-two-column-in-r count &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), nrow) names(count) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_freq&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr count &lt;- count %&gt;% spread(cultural_venue_type, culture_freq) #----then use the rowSums function to sum up the counts from all cultural venue type, skip NA values and create new column count$culture_freq &lt;- rowSums(count[,sapply(count, is.numeric)], na.rm=TRUE) return(count) } JoinCulture_rating &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average rating of cultural venue types per LSOA #https://stackoverflow.com/questions/11562656/calculate-the-mean-by-group average &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), function(x) mean(x$rating)) names(average) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_rating&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr average &lt;- average %&gt;% spread(cultural_venue_type, culture_rating) #----then use the rowSums function to average the ratings from all cultural venue type, skip NA values and create new column average$culture_rating &lt;- rowMeans(average[,sapply(average, is.numeric)], na.rm=TRUE) return(average) } JoinCulture_NOreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the sum of reviews of cultural venue types per LSOA #https://stackoverflow.com/questions/11562656/calculate-the-mean-by-group count &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), function(x) sum(x$user_ratings_total)) names(count) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_no_reviews&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr count &lt;- count %&gt;% spread(cultural_venue_type, culture_no_reviews) #----then use the rowSums function to sum up the reviews from all cultural venue type, skip NA values and create new column count$culture_no_reviews &lt;- rowSums(count[,sapply(count, is.numeric)], na.rm=TRUE) return(count) } JoinCulture_AVreviews &lt;- function(data1, data2) { #----join dataframes joined &lt;- st_join(data1, data2, join = st_within) #----calculate the average rating of cultural venue types per LSOA #https://stackoverflow.com/questions/11562656/calculate-the-mean-by-group average &lt;- ddply(joined, .(joined$LSOA11CD, joined$Cultural.Venue.Type), function(x) mean(x$user_ratings_total)) names(average) &lt;- c(&quot;LSOA11CD&quot;, &quot;cultural_venue_type&quot;, &quot;culture_av_reviews&quot;) #----then use the spread function from the tidyr lib to turn long data into wide data - i.e. a column for each cultural venue type #source----https://uc-r.github.io/tidyr average &lt;- average %&gt;% spread(cultural_venue_type, culture_av_reviews) #----then use the rowSums function to average the reviews from all cultural venue type, skip NA values and create new column average$culture_av_reviews &lt;- rowMeans(average[,sapply(average, is.numeric)], na.rm=TRUE) return(average) } #----//////////////////////////////////////////////////////////////////////// #----CULTURE COUNT----# #----use the first function to count the number for each cultural venue category in each LSOA cultureLSOA_count &lt;- JoinCulture_count(culture, londonLSOA) #----CULTURE RATING----# #----use the second function to calculate the average rating of Google Places reviews for each cultural venue category in each LSOA cultureLSOA_rating &lt;- JoinCulture_rating(culture, londonLSOA) #----CULTURE NUMBER OF REVIEWS----# #----use the third function to count the number of Google Places reviews for each cultural venue category in each LSOA cultureLSOA_review_count &lt;- JoinCulture_NOreviews(culture, londonLSOA) #----CULTURE AVERAGE NUMBER OF REVIEWS----# #----use the third function to count the number of Google Places reviews for each cultural venue category in each LSOA cultureLSOA_review_average &lt;- JoinCulture_AVreviews(culture, londonLSOA) 3. Join Airbnb, Culture and LSOA shapefile data Having done the previous calculations for Airbnb and Cultural Infrastructure per LSOA, all the data is joined together. #----merge all dataframes into one #https://stackoverflow.com/questions/8091303/simultaneously-merge-multiple-data-frames-in-a-list #install safejoin package from GitHub devtools::install_github(&quot;moodymudskipper/safejoin&quot;) library(safejoin) #----use eat function from safejoin to merge a list of all the dataframes londonLSOAextradata &lt;- eat(airbnbLSOA_count, list(airbnbLSOA_price, airbnbLSOA_review_count, airbnbLSOA_review_average, cultureLSOA_count, cultureLSOA_rating, cultureLSOA_review_count, cultureLSOA_review_average), .by = &quot;LSOA11CD&quot;, .conflict = ~.x) 4. Join LSOA profile data to the rest Now that we have all of our data, we join them all together to create the londonLSOAProfiles sf object. Moreover, we caclulate the frequency density of cultural venue type per km^2 in every LSAO as per #----merge the LSOA boundaries shapefile with the and LSOA attribute dataframe londonLSOAProfiles &lt;- inner_join(londonLSOA, londonLSOAProfiles, by = c(&quot;LSOA11CD&quot; = &quot;Lower Super Output Area&quot;)) ## Warning: Column `LSOA11CD`/`Lower Super Output Area` joining factor and ## character vector, coercing into character vector #londonLSOAProfiles &lt;- na.omit(londonLSOAProfiles) #----join the extra data - Airbnb price &amp; counts and culture counts - to the LSAO profile data londonLSOAProfiles &lt;- inner_join(londonLSOAProfiles, londonLSOAextradata, by = c(&#39;LSOA11CD&#39;)) ## Warning: Column `LSOA11CD` joining character vector and factor, coercing into ## character vector 5. Join Inner/Outer London boundaries with the data Inner/Outer London boundaries join with dataset. #----join the shapefile to the LSOA SF obbject on geometry inner_outer_df &lt;- st_join(londonLSOA, inner_outerSF, by = c(&quot;geometry&quot; = &quot;geometry&quot;)) inner_outer_df &lt;- as.data.frame(inner_outer_df) names(inner_outer_df)[names(inner_outer_df) == &#39;Boundary&#39;] &lt;- &#39;InnerOuter&#39; select.me &lt;- c(&#39;LSOA11CD&#39;,&#39;InnerOuter&#39;) inner_outer_df &lt;- inner_outer_df[,select.me] #----join the Inner and Outer London extra data to the LSAO profile data londonLSOAProfiles &lt;- inner_join(londonLSOAProfiles, inner_outer_df, by = c(&quot;LSOA11CD&quot; = &quot;LSOA11CD&quot;)) ## Warning: Column `LSOA11CD` joining character vector and factor, coercing into ## character vector #----drop duplicate rows for LSOA11CD, culture_freq, airbnb_price, airbnb_freq columns londonLSOAProfiles &lt;- londonLSOAProfiles[!duplicated(londonLSOAProfiles[c(&quot;LSOA11CD&quot;, &quot;culture_freq&quot;, &quot;airbnb_price&quot;, &quot;airbnb_freq&quot;)]),] #londonLSOAProfiles &lt;- londonLSOAProfiles[londonLSOAProfiles$InnerOuter == &#39;Inner London&#39;,] 3.4 Clean variables before analysis Some variable names are changed before the analysis for ease of use and also to align with Dudas et al (2017). #----change the column names of some of the independent variables colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Population Density;Area (Hectares);&quot;)] &lt;- &quot;areaLSOA&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Population Density;Persons per hectare;2013&quot;)] &lt;- &quot;pop_density&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Ethnic Group;BAME (%);2011&quot;)] &lt;- &quot;bame_p&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Country of Birth;% Not United Kingdom;2011&quot;)] &lt;- &quot;nonUK&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Tenure;Owned outright (%);2011&quot;)] &lt;- &quot;house_own&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Tenure;Owned with a mortgage or loan (%);2011&quot;)] &lt;- &quot;house_mortg&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;House Prices;Median Price (£);2014&quot;)] &lt;- &quot;house_price&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Economic Activity;Employment Rate;2011&quot;)] &lt;- &quot;employees&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Qualifications;% Highest level of qualification: Level 4 qualifications and above;2011&quot;)] &lt;- &quot;education&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Household Income, 2011/12;Median Annual Household Income estimate (£)&quot;)] &lt;- &quot;income&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Public Transport Accessibility Levels (2014);% 4-6 (good access)&quot;)] &lt;- &quot;transport&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;2013 Census Population;Age Structure;16-29&quot;)] &lt;- &quot;age16_29&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;2014 Census Population;Age Structure;30-44&quot;)] &lt;- &quot;age30_44&quot; colnames(londonLSOAProfiles)[which(names(londonLSOAProfiles) == &quot;Dwelling type;All Households;2011&quot;)] &lt;- &quot;housing&quot; #----change the some of the independent variables to density values by dividing by the area of the LSOA in hectares - areaLSOA londonLSOAProfiles$young_p &lt;- ((londonLSOAProfiles$age16_29 + londonLSOAProfiles$age30_44)/londonLSOAProfiles$areaLSOA)*100 londonLSOAProfiles$housing &lt;- (londonLSOAProfiles$housing/londonLSOAProfiles$areaLSOA)*100 #----calculate the km^2 frequency density of Airbnb per LSAO (as per Dudas et al, 2017) londonLSOAProfiles$airbnb_freq &lt;- (londonLSOAProfiles$airbnb_freq/londonLSOAProfiles$areaLSOA)*100 londonLSOAProfiles$airbnb_no_reviews &lt;- (londonLSOAProfiles$airbnb_no_reviews/londonLSOAProfiles$areaLSOA)*100 #----find the column numbers for first and last cultural venue type which( colnames(londonLSOAProfiles)==&quot;Archives&quot; ) ## [1] 33 which( colnames(londonLSOAProfiles)==&quot;culture_freq&quot; ) ## [1] 66 #----function to change variable from freqency to freqency density per km2 by dividing by the LSOA hectare size and muliplying by 100 A &lt;- function(x) (x / londonLSOAProfiles$`areaLSOA`)*100 #----calculate the km^2 frequency density of cultural venue type per LSAO (as per Dudas et al, 2017) londonLSOAProfiles[33:66] &lt;- lapply(londonLSOAProfiles[33:66], A) ## Warning in `[&lt;-.data.frame`(`*tmp*`, 33:66, value = list(Archives = ## c(23.0769230769231, : provided 35 variables to replace 34 variables Create Dummy Variables for the rating and reviews of cultural infrastructure from Airbnb #https://support.google.com/business/answer/4801187?hl=en-GB rating criteria londonLSOAProfiles$culture_rating_good &lt;- ifelse(as.numeric(londonLSOAProfiles$culture_rating) &gt;= 4, 1, 0) quantile(londonLSOAProfiles$culture_av_reviews, 0.80, na.rm=TRUE) ## 80% ## 437.0026 londonLSOAProfiles$culture_reviews_popular &lt;- ifelse(as.numeric(londonLSOAProfiles$culture_av_reviews) &gt;= 400, 1, 0) # londonLSOAProfiles &lt;- londonLSOAProfiles %&gt;% # mutate(culture_rating_loved = as.numeric(culture_rating &gt;= quantile(culture_rating, 0.90, na.rm=TRUE)), # culture_rating_liked = as.numeric(culture_rating &gt;= quantile(culture_rating, 0.75, na.rm=TRUE) &amp; culture_rating &lt; quantile(culture_rating, 0.90, na.rm=TRUE)), # culture_rating_okay = as.numeric(culture_rating &gt;= quantile(culture_rating, 0.50, na.rm=TRUE) &amp; culture_rating &lt; quantile(culture_rating, 0.75, na.rm=TRUE)), # culture_rating_disliked = as.numeric(culture_rating &gt;= quantile(culture_rating, 0.25, na.rm=TRUE) &amp; culture_rating &lt; quantile(culture_rating, 0.50, na.rm=TRUE)), # culture_rating_hated = as.numeric(culture_rating &lt;= quantile(culture_rating, 0.25, na.rm=TRUE))) # londonLSOAProfiles &lt;- londonLSOAProfiles %&gt;% # mutate(culture_rating_good = as.numeric(culture_rating &gt;= quantile(culture_rating, 0.50, na.rm=TRUE)), # culture_rating_bad = as.numeric(culture_rating &lt; quantile(culture_rating, 0.50, na.rm=TRUE))) "],
["exploratory-data-analysis.html", "Chapter 4 Exploratory Data Analysis 4.1 Airbnb EDA 4.2 Cultural infrastructure EDA", " Chapter 4 Exploratory Data Analysis Having retrieved all the necessary data, we now perform some exploratory data analysis on the variables. Before doing the EDA, we load in some very useful data visualization libraries used by this book Fundementals of Data Visualization - https://serialmentor.com/dataviz/geospatial-data.html #----load all the libraries needed # load in libraries library(tidyverse) library(scales) library(lubridate) library(ggridges) library(gridExtra) #----data visualization packages - https://serialmentor.com/dataviz/geospatial-data.html #install.packages(&quot;remotes&quot;) library(remotes) #devtools::install_github(&quot;wilkelab/cowplot&quot;) library(cowplot) #install.packages(&quot;colorspace&quot;) library(colorspace) #devtools::install_github(&quot;clauswilke/colorblindr&quot;) #https://rdrr.io/github/clauswilke/dviz.supp/ #devtools::install_github(&quot;clauswilke/dviz.supp&quot;) library(dviz.supp) #----good bblog post on the formattable package: https://www.littlemissdata.com/blog/prettytables #install.packages(&quot;data.table&quot;) #install.packages(&quot;dplyr&quot;) #install.packages(&quot;formattable&quot;) #install.packages(&quot;tidyr&quot;) library(data.table) library(dplyr) library(formattable) library(tidyr) #Zivkovic (2019) Great Kaggle Kernel on EDA - https://www.kaggle.com/jaseziv83/a-deep-dive-eda-into-all-variables options(scipen = 999) Then, we set up some basic settings from this great Kaggle Kernel by X for data visualisation of exploratory data analysis of variables https://www.kaggle.com/jaseziv83/a-deep-dive-eda-into-all-variables/report #----set the plotting theme baseline from Zivkovic (2019) theme_set(theme_minimal() + theme(axis.title.x = element_text(size = 15, hjust = 1), axis.title.y = element_text(size = 15), axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), panel.grid.major = element_line(linetype = 2), panel.grid.minor = element_line(linetype = 2), plot.margin=unit(c(1,1,-0.5,1),&quot;cm&quot;), plot.title = element_text(size = 18, colour = &quot;grey25&quot;, face = &quot;bold&quot;), plot.subtitle = element_text(size = 16, colour = &quot;grey44&quot;))) #----load the colours from Zivkovic (2019) col_pal &lt;- c(&quot;#5EB296&quot;, &quot;#4E9EBA&quot;, &quot;#F29239&quot;, &quot;#C2CE46&quot;, &quot;#FF7A7F&quot;, &quot;#4D4D4D&quot;) 4.1 Airbnb EDA First we do EDA on the Airbnb data londonLSOAProfiles_nogeom &lt;- st_set_geometry(londonLSOAProfiles, NULL) #----use code from blog post to create a formattable table: https://www.littlemissdata.com/blog/prettytables a1 &lt;- londonLSOAProfiles_nogeom %&gt;% group_by(LAD11NM) %&gt;% summarise( airbnb_freq=mean(airbnb_freq), airbnb_no_reviews=mean(airbnb_no_reviews), airbnb_price=mean(airbnb_price), culture_freq=mean(culture_freq, na.rm=TRUE), culture_rating=mean(culture_rating, na.rm=TRUE), culture_av_reviews=mean(culture_av_reviews, na.rm=TRUE) ) %&gt;% arrange(desc(airbnb_freq)) %&gt;% top_n(n = 11, wt = airbnb_freq) options(digits = 3) formattable(a1) LAD11NM airbnb_freq airbnb_no_reviews airbnb_price culture_freq culture_rating culture_av_reviews Westminster 680 9157 113.1 45.6 4.28 1012 Kensington and Chelsea 568 7125 122.6 23.8 4.22 693 Tower Hamlets 534 6716 73.2 30.6 4.29 464 Camden 385 5573 98.1 45.1 4.25 504 Hammersmith and Fulham 367 4368 88.9 26.5 4.28 309 Hackney 342 3699 73.1 38.7 4.27 217 Islington 341 4572 85.7 35.1 4.27 280 City of London 280 3417 128.9 92.0 4.39 883 Southwark 246 3554 77.9 24.8 4.27 406 Lambeth 211 2990 71.3 24.4 4.21 280 Wandsworth 141 1354 80.7 17.0 4.22 346 4.1.1 Airbnb freqency distributions When looking at the distribution of Airbnb listings we see that there are some outliters #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$airbnb_freq), 0) plot1 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(airbnb_freq &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=airbnb_freq)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;AIRBNB FREQUENCY&quot;, subtitle = &quot;Airbnb freqency still skewed even for non-outliers&quot;) + labs(x= &quot;Airbnb freq per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$airbnb_price), 0) plot2 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(airbnb_price &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=airbnb_price)) + geom_histogram(alpha = 0.5, fill = col_pal[2], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;AIRBNB PRICE&quot;, subtitle = &quot;Airbnb price still skewed even for non-outliers&quot;) + labs(x= &quot;Airbnb price per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$airbnb_av_reviews), 0) plot3 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(airbnb_av_reviews &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=airbnb_av_reviews)) + geom_histogram(alpha = 0.5, fill = col_pal[3], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;AIRBNB REVIEWS&quot;, subtitle = &quot;Airbnb reviews still skewed even for non-outliers&quot;) + labs(x= &quot;Airbnb reviews per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) g &lt;- grid.arrange(plot1, plot2, plot3, ncol=3) #ggsave(&quot;graphs/1.png&quot;, plot = g, width = 10, height = 4) 4.1.2 Airbnb log freqency distributions Lets look at the log of Airbnb freqency #----this log distribution function was taken from Zivkovic (2019) plot4 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_freq))) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG AIRBNB FREQ&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;, y= &quot;Count&quot;) #----this log distribution function was taken from Zivkovic (2019) plot5 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_price))) + geom_histogram(alpha = 0.5, fill = col_pal[2], colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG AIRBNB PRICE&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb price)&quot;, y= &quot;Count&quot;) #----this log distribution function was taken from Zivkovic (2019) plot6 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_no_reviews))) + geom_histogram(alpha = 0.5, fill = col_pal[3], colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG AIRBNB REVIEWS&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb reviews)&quot;, y= &quot;Count&quot;) grid.arrange(plot4, plot5, plot6, ncol=3) 4.1.3 Airbnb Inner vs Outer London Lets look at Airbnb frequency in Inner vs Outer London #----this log distribution function was taken from Zivkovic (2019) plot7 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_freq), fill = as.character(InnerOuter))) + geom_density(alpha = 0.5, adjust = 2) + scale_fill_manual(values = col_pal) + ggtitle(&quot;THERE ARE MORE LISTINGS IN INNER LONDON&quot;, subtitle = &quot;&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;) + theme(axis.title.y = element_blank(), legend.position = &quot;top&quot;) #----this log distribution function was taken from Zivkovic (2019) plot8 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_price), fill = as.character(InnerOuter))) + geom_density(alpha = 0.5, adjust = 2) + scale_fill_manual(values = col_pal) + ggtitle(&quot;LISTINGS ARE MORE EXPENSIVE IN INNER LONDON&quot;, subtitle = &quot;&quot;) + labs(x= &quot;log(Airbnb Price)&quot;) + theme(axis.title.y = element_blank(), legend.position = &quot;top&quot;) #----this log distribution function was taken from Zivkovic (2019) plot9 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(airbnb_no_reviews), fill = as.character(InnerOuter))) + geom_density(alpha = 0.5, adjust = 2) + scale_fill_manual(values = col_pal) + ggtitle(&quot;LISTINGS ARE MORE EXPENSIVE IN INNER LONDON&quot;, subtitle = &quot;&quot;) + labs(x= &quot;log(Airbnb Price)&quot;) + theme(axis.title.y = element_blank(), legend.position = &quot;top&quot;) grid.arrange(plot7, plot8, plot9, ncol=3) 4.2 Cultural infrastructure EDA Now lets look at cultural infrastructure 4.2.1 Cultural infrastructure frequency distributions #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$culture_freq, na.rm = TRUE), 0) plot10 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(culture_freq &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=culture_freq)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;CULTURAL INFRASTRUCUTRE IN FREQUENCY&quot;, subtitle = &quot;Cultural Infrastructure freqency still skewed even for non-outliers&quot;) + labs(x= &quot;Cultural Infrastructure freq per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$culture_rating, na.rm = TRUE), 0) plot11 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(culture_rating &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=culture_rating)) + geom_histogram(alpha = 0.5, fill = col_pal[2], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;CULTURAL INFRASTRUCUTRE Rating&quot;, subtitle = &quot;Cultural Infrastructure price still skewed even for non-outliers&quot;) + labs(x= &quot;Cultural Infrastructure price per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) #----this distribution function was taken from Zivkovic (2019) outlier &lt;- round(1.5 * IQR(londonLSOAProfiles$culture_no_reviews, na.rm = TRUE), 0) plot12 &lt;- londonLSOAProfiles %&gt;% mutate(outlier = ifelse(culture_no_reviews &gt; outlier, &quot;Outlier&quot;, &quot;Not Outlier&quot;)) %&gt;% ggplot(aes(x=culture_no_reviews)) + geom_histogram(alpha = 0.5, fill = col_pal[3], colour = &quot;#4D4D4D&quot;) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma) + ggtitle(&quot;CULTURAL INFRASTRUCUTRE REVIEWS&quot;, subtitle = &quot;Cultural Infrastructure reviews still skewed even for non-outliers&quot;) + labs(x= &quot;Cultural Infrastructure reviews per LSOA&quot;, y= &quot;Count&quot;) + facet_wrap(~ outlier, scales = &quot;free&quot;) grid.arrange(plot10, plot11, plot12, ncol=3) ## Warning: Removed 2116 rows containing non-finite values (stat_bin). ## Warning: Removed 2116 rows containing non-finite values (stat_bin). ## Warning: Removed 2116 rows containing non-finite values (stat_bin). 4.2.2 Cultural infrastructure log distibutions #----this log distribution function was taken from Zivkovic (2019) plot13 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(culture_freq))) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG CULTURE FREQ&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;, y= &quot;Count&quot;) #----this log distribution function was taken from Zivkovic (2019) plot14 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= culture_rating)) + geom_histogram(alpha = 0.5, fill = col_pal[2], colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG CULTURE RATINGS&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb price)&quot;, y= &quot;Count&quot;) #----this log distribution function was taken from Zivkovic (2019) plot15 &lt;- londonLSOAProfiles %&gt;% ggplot(aes(x= log(culture_no_reviews))) + geom_histogram(alpha = 0.5, fill = col_pal[3], colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;LOG CULTURE REVIEWS&quot;, subtitle = &quot;The variable looks a lot more workable now&quot;) + labs(x= &quot;log(Airbnb reviews)&quot;, y= &quot;Count&quot;) grid.arrange(plot13, plot14, plot15, ncol=3) ## Warning: Removed 2116 rows containing non-finite values (stat_bin). ## Warning: Removed 2116 rows containing non-finite values (stat_bin). ## Warning: Removed 2116 rows containing non-finite values (stat_bin). 4.2.3 All variables Freqency distributions #Use code from MacLachlan &amp; Dennett (2019: chapter 8.2.4) to visualise frequency of dependent variables - https://andrewmaclachlan.github.io/CASA0005repo/online-mapping-descriptive-statistics.html#learning-objectives-1 #----find the column numbers for first and last cultural venue type which( colnames(londonLSOAProfiles)==&quot;bame_p&quot; ) ## [1] 17 which( colnames(londonLSOAProfiles)==&quot;housing&quot; ) ## [1] 28 #----check which variables are numeric first list1 &lt;- as.data.frame(cbind(lapply(londonLSOAProfiles, class))) list1 &lt;- cbind(list1, seq.int(nrow(list1))) londonSub &lt;- londonLSOAProfiles[,c(1:2, 17:28)] #----drop geometry column londonSub &lt;- st_set_geometry(londonSub, NULL) library(reshape2) #----reshape the dataframe with LSOA name and ID as indicies londonSub &lt;- melt(londonSub, id.vars = 1:2) attach(londonSub) #----plot the histograms hist2 &lt;- ggplot(londonSub, aes(x=value)) + geom_histogram(aes(y = ..density..)) + geom_density(colour=&quot;red&quot;, size=1, adjust=1) hist2 + facet_wrap(~ variable, scales=&quot;free&quot;) ## Warning: Removed 2 rows containing non-finite values (stat_bin). ## Warning: Removed 2 rows containing non-finite values (stat_density). Correlations #----relationship between dependent variables (Inner vs Outer London) ggplot(londonLSOAProfiles, aes(log(airbnb_freq), log(airbnb_price), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;airbnb freq per LSOA (km2)&quot;) + scale_y_continuous(name = &quot;airbnb price per LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) Relationship between culture and airbnb reviews (Inner vs Outer London) #----relationship between culture and airbnb reviews (Inner vs Outer London) ggplot(londonLSOAProfiles, aes(log(culture_no_reviews), log(airbnb_no_reviews), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;culture_no_reviews PER LSOA (km2)&quot;) + scale_y_continuous(name = &quot;airbnb_no_reviews LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) ## Warning: Removed 2116 rows containing missing values (geom_point). Relationship between culture and airbnb frequency (Inner vs Outer London) #----relationship between culture and airbnb frequency (Inner vs Outer London) ggplot(londonLSOAProfiles, aes(log(culture_no_reviews), log(airbnb_freq), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;culture_no_reviews PER LSOA (km2)&quot;) + scale_y_continuous(name = &quot;airbnb_freq LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) ## Warning: Removed 2116 rows containing missing values (geom_point). Relationship between culture and airbnb price (Inner vs Outer London) #----relationship between culture and airbnb price (Inner vs Outer London) ggplot(londonLSOAProfiles, aes(log(culture_no_reviews), log(airbnb_price), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;culture_no_reviews PER LSOA (km2)&quot;) + scale_y_continuous(name = &quot;airbnb_price LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) ## Warning: Removed 2116 rows containing missing values (geom_point). ggplot(londonLSOAProfiles, aes(log(culture_no_reviews), log(house_price), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;culture_no_reviews PER LSOA (km2)&quot;) + scale_y_continuous(name = &quot;house_price LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( #legend.position = c(1, 0.01), #legend.justification = c(1, 0), legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) ## Warning: Removed 2118 rows containing missing values (geom_point). ggplot(londonLSOAProfiles, aes(log(culture_freq), log(house_price), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;culture_freq PER LSOA (km2)&quot;) + scale_y_continuous(name = &quot;house_price LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( #legend.position = c(1, 0.01), #legend.justification = c(1, 0), legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) ## Warning: Removed 2118 rows containing missing values (geom_point). ggplot(londonLSOAProfiles, aes(culture_rating, log(house_price), fill = InnerOuter)) + geom_point(pch = 21, color = &quot;white&quot;, size = 2.5) + scale_x_continuous(name = &quot;culture_rating PER LSOA (km2)&quot;) + scale_y_continuous(name = &quot;house_price LSOA (km2)&quot;) + scale_fill_manual( values = c(&#39;Inner London&#39; = &quot;#D55E00&quot;, &#39;Outer London&#39; = &quot;#0072B2&quot;), breaks = c(&quot;F&quot;, &quot;M&quot;), labels = c(&quot;female birds &quot;, &quot;male birds&quot;), name = NULL, guide = guide_legend( direction = &quot;horizontal&quot;, override.aes = list(size = 3) ) ) + theme_dviz_grid() + theme( #legend.position = c(1, 0.01), #legend.justification = c(1, 0), legend.position = &quot;top&quot;, legend.justification = &quot;right&quot;, legend.box.spacing = unit(3.5, &quot;pt&quot;), # distance between legend and plot legend.text = element_text(vjust = 0.6), legend.spacing.x = unit(2, &quot;pt&quot;), legend.background = element_rect(fill = &quot;white&quot;, color = NA), legend.key.width = unit(10, &quot;pt&quot;) ) ## Warning: Removed 2118 rows containing missing values (geom_point). #select some variables from the data file myvars &lt;- c(&quot;airbnb_price&quot;, &quot;airbnb_freq&quot;, &#39;young_p&#39;, &#39;pop_density&#39;, &#39;bame_p&#39;, &#39;nonUK&#39;, &#39;education&#39;, &#39;employees&#39;, &#39;income&#39;, &#39;housing&#39;, &#39;house_own&#39;, &#39;house_mortg&#39;, &#39;house_price&#39;, &#39;transport&#39;, &quot;culture_freq&quot;, &quot;culture_rating&quot;) #Extracting data.frame from simple features object in R - https://gis.stackexchange.com/questions/224915/extracting-data-frame-from-simple-features-object-in-r x &lt;- londonLSOAProfiles[myvars] st_geometry(x) &lt;- NULL class(x) ## [1] &quot;data.frame&quot; #check their correlations are OK #install.packages(&quot;Hmisc&quot;) #http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software library(&quot;Hmisc&quot;) cormat &lt;- rcorr(as.matrix(x), type = c(&quot;spearman&quot;)) library(corrplot) # Insignificant correlations are leaved blank corrplot(cormat$r, type=&quot;upper&quot;, order=&quot;hclust&quot;, p.mat = cormat$S, sig.level = 0.1, insig = &quot;blank&quot;) "],
["maps.html", "Chapter 5 Maps 5.1 Culture dot map 5.2 Culture frequency density map 5.3 Culture rating density map 5.4 Airbnb freq density map - Supply &amp; Demand", " Chapter 5 Maps #----load all the libraries needed library(tmap) #http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf 5.1 Culture dot map londonLSOAProfiles_inner &lt;- londonLSOAProfiles[londonLSOAProfiles$InnerOuter == &#39;Inner London&#39;,] culture_dots_london &lt;- tm_shape(londonLSOAProfiles) + tm_fill(col = &#39;white&#39;) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons() + tm_shape(culture[londonLSOAProfiles, ]) + tm_dots(col = &quot;Cultural.Venue.Type&quot;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Greater London&quot;, position = c(0, 0.9), size=0.7) culture_dots_inner_london &lt;- tm_shape(londonLSOAProfiles_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons() + tm_shape(culture[londonLSOAProfiles_inner,]) + tm_dots(col = &quot;Cultural.Venue.Type&quot;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Inner London&quot;, position = c(0.1, 0.9), size=0.7) legend1 &lt;- tm_shape(londonLSOAProfiles) + tm_polygons() + tm_shape(culture[londonLSOAProfiles,]) + tm_dots(col = &quot;Cultural.Venue.Type&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.6), asp=0, legend.stack = &quot;horizontal&quot;, legend.width = 1 #legend.outside = TRUE, legend.outside.position = &quot;bottom&quot; ) + tm_credits(&quot;Source: data.london - Cultural Infrasturcture Map&quot;, position=c(0.1, 0.3)) culture_dot_map = tmap_arrange(culture_dots_london, culture_dots_inner_london, legend1, ncol=2) #culture_dot_map #tmap_save(culture_dot_map, filename = &quot;maps/density/cultural_infrastructure_dots_lsoa.png&quot;) 5.2 Culture frequency density map culture_freq_london &lt;- tm_shape(londonLSOAProfiles) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;culture_freq&#39;, style = &#39;log10&#39;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7 ) culture_freq_inner_london &lt;- tm_shape(londonLSOAProfiles_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;culture_freq&#39;,style = &#39;log10&#39;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) legend1 &lt;- tm_shape(londonLSOAProfiles) + tm_polygons(title = &quot;Culture Freq (km2)&quot;, col = &quot;culture_freq&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.6), asp=0.1) + tm_credits(&quot;Source: data.london - Cultural Infrasturcture Map&quot;, position=c(0.1, 0.3)) legend2 &lt;- tm_shape(londonLSOAProfiles_inner) + tm_polygons(title = &quot;Culture Freq (km2)&quot;, col =&quot;culture_freq&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(title = &quot;Cultural Freq&quot;, legend.only = TRUE, legend.position=c(0.6, 0.6), asp=0.1) #+ tm_credits(&quot;Source: data.london - Cultural Infrasturcture Map&quot;, position=c(0, 0.3)) culture_freq_map = tmap_arrange(culture_freq_london, culture_freq_inner_london, legend1, legend2, ncol=2) #tmap_save(culture_freq_map, filename = &quot;maps/density/cultural_infrastructure_density_lsoa.png&quot;) culture_freq_map 5.3 Culture rating density map culture_rating_london &lt;- tm_shape(londonLSOAProfiles) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;culture_rating&#39;, style = &#39;quantile&#39;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7 ) culture_rating_inner_london &lt;- tm_shape(londonLSOAProfiles_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;culture_rating&#39;, style = &#39;quantile&#39;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) legend1 &lt;- tm_shape(londonLSOAProfiles) + tm_polygons(title = &quot;Culture Rating&quot;, col = &quot;culture_rating&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.4), asp=0.1) + tm_credits(&quot;Source: data.london - Cultural Infrasturcture Map&quot;, position=c(0.1, 0.3)) legend2 &lt;- tm_shape(londonLSOAProfiles_inner) + tm_polygons(title = &quot;Culture Rating&quot;, col =&quot;culture_rating&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.4), asp=0.1) #+ tm_credits(&quot;Source: data.london - Cultural Infrasturcture Map&quot;, position=c(0, 0.3)) culture_rating_map = tmap_arrange(culture_rating_london, culture_rating_inner_london, legend1, legend2, ncol=2) #tmap_save(culture_rating_map, filename = &quot;maps/density/cultural_infrastructure_rating_lsoa.png&quot;) culture_rating_map 5.4 Airbnb freq density map - Supply &amp; Demand #----------------------------------- airbnb_freq_london &lt;- tm_shape(londonLSOAProfiles) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;airbnb_freq&#39;, style = &#39;order&#39;) + tm_layout(title = &#39;Supply&#39;, frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7 ) airbnb_freq_inner_london &lt;- tm_shape(londonLSOAProfiles_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;airbnb_freq&#39;, style = &#39;order&#39;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) legend1 &lt;- tm_shape(londonLSOAProfiles) + tm_polygons(title = &quot;Airbnb Freq (km2)&quot;, col = &quot;airbnb_freq&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.4), asp=0.1) + tm_credits(&quot;Source: Inside Airbnb&quot;, position=c(0.1, 0.3)) legend2 &lt;- tm_shape(londonLSOAProfiles_inner) + tm_polygons(title = &quot;Airbnb Freq (km2)&quot;, col =&quot;airbnb_freq&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.4), asp=0.1) #+ tm_credits(&quot;Source: data.london - Cultural Infrasturcture Map&quot;, position=c(0, 0.3)) airbnb_freq_map = tmap_arrange(airbnb_freq_london, airbnb_freq_inner_london, legend1, legend2, ncol=2) #tmap_save(airbnb_freq_map, filename = &quot;maps/density/airbnb_freq_lsoa.png&quot;) #----------------------------------- airbnb_reviews_london &lt;- tm_shape(londonLSOAProfiles) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;airbnb_no_reviews&#39;, style = &#39;order&#39;) + tm_layout(title = &#39;Demand&#39;, frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7 ) airbnb_reviews_inner_london &lt;- tm_shape(londonLSOAProfiles_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.3, lty = &quot;solid&quot;) + tm_polygons(col = &#39;airbnb_no_reviews&#39;, style = &#39;order&#39;) + tm_layout(frame=FALSE) + tm_legend(show =FALSE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) legend3 &lt;- tm_shape(londonLSOAProfiles) + tm_polygons(title = &quot;Airbnb Reviews (km2)&quot;, col = &quot;airbnb_no_reviews&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.4), asp=0.1) + tm_credits(&quot;Source: Inside Airbnb&quot;, position=c(0.1, 0.3)) legend4 &lt;- tm_shape(londonLSOAProfiles_inner) + tm_polygons(title = &quot;Airbnb Reviews (km2)&quot;, col =&quot;airbnb_no_reviews&quot;) + tm_scale_bar(position=c(0.2, 0.6), text.size=0.6) + tm_compass(north=0, position=c(0.2, 0.8)) + tm_layout(legend.only = TRUE, legend.position=c(0.6, 0.4), asp=0.1) #+ tm_credits(&quot;Source: data.london - Cultural Infrasturcture Map&quot;, position=c(0, 0.3)) airbnb_reviews_map = tmap_arrange(airbnb_reviews_london, airbnb_reviews_inner_london, legend3, legend4, ncol=2) tmap_save(airbnb_reviews_map, filename = &quot;maps/density/airbnb_reviews_lsoa.png&quot;) airbnb_reviews_map "],
["regression.html", "Chapter 6 Regression 6.1 Gloabl Regression Models 6.2 Spatial lag regression model 6.3 Geographically Weighted Regression Models (GWR)", " Chapter 6 Regression Now we are going to runa regression on #----load all the libraries needed # load in libraries library(tidyverse) library(scales) library(lubridate) library(ggridges) library(gridExtra) #regression library(corrplot) library(rgdal) library(spdep) library(car) #----data visualization packages - https://serialmentor.com/dataviz/geospatial-data.html #install.packages(&quot;remotes&quot;) #install.packages(&quot;devtools&quot;) library(remotes) #install.packages(&quot;cowplot&quot;) #devtools::install_github(&quot;wilkelab/cowplot&quot;) library(cowplot) #install.packages(&quot;colorspace&quot;) library(colorspace) #devtools::install_github(&quot;clauswilke/colorblindr&quot;) #https://rdrr.io/github/clauswilke/dviz.supp/ #devtools::install_github(&quot;clauswilke/dviz.supp&quot;) library(dviz.supp) #https://cran.r-project.org/web/packages/jtools/vignettes/summ.html #install.packages(&#39;jtools&#39;) library(jtools) options(scipen = 999) 6.1 Gloabl Regression Models #---read InnerOuter variable as a factor londonLSOAProfiles$InnerOuter &lt;- as.factor(londonLSOAProfiles$InnerOuter) #---SUPPLY SIDE MODEL modelFreq &lt;- lm(log(`airbnb_freq`) ~ `bame_p` + log(`young_p`) + `nonUK` + `education` + `income` + `house_mortg` + `house_price` + log(culture_freq) + #log(culture_rating) + `culture_rating_good` + `culture_reviews_popular` + `InnerOuter`, data = londonLSOAProfiles, na.action=na.exclude) #---DEMAND SIDE MODEL modelReviews &lt;- lm(log(`airbnb_no_reviews`) ~ `bame_p` + log(`young_p`) + `nonUK` + `education` + `employees` + log(`income`) + `house_mortg` + `house_price` + log(`culture_freq`) + `culture_rating_good` + `culture_reviews_popular` + `InnerOuter`, data = londonLSOAProfiles, na.action=na.exclude) #---AIRBNB PRICE MODEL modelPrice &lt;- lm(log(`airbnb_price`) ~ `bame_p` + `nonUK` + `education` + `employees` + `income` + `house_mortg` + `house_price` + log(culture_freq) + `culture_rating_good` + `culture_reviews_popular` + `InnerOuter`, data = londonLSOAProfiles, na.action=na.exclude) Dudas et al said remove all variables where vif is above 5 print(vif(modelFreq)) ## bame_p log(young_p) nonUK ## 4.54 2.69 4.06 ## education income house_mortg ## 4.98 6.79 2.97 ## house_price log(culture_freq) culture_rating_good ## 2.76 2.56 1.04 ## culture_reviews_popular InnerOuter ## 1.10 2.02 print(vif(modelReviews)) ## bame_p log(young_p) nonUK ## 4.89 2.73 4.07 ## education employees log(income) ## 8.06 3.78 8.65 ## house_mortg house_price log(culture_freq) ## 3.85 2.56 2.56 ## culture_rating_good culture_reviews_popular InnerOuter ## 1.05 1.10 2.04 print(vif(modelPrice)) ## bame_p nonUK education ## 4.81 4.00 6.72 ## employees income house_mortg ## 3.79 6.55 3.50 ## house_price log(culture_freq) culture_rating_good ## 2.80 1.81 1.04 ## culture_reviews_popular InnerOuter ## 1.07 1.93 Run the regression again without the high vif variable and insignificant variables #---SUPPLY SIDE MODEL modelFreq &lt;- lm(log(`airbnb_freq`) ~ `bame_p` + log(`young_p`) + `nonUK` + `education` + `house_mortg` + `house_price` + log(culture_freq) + `culture_rating_good` + `InnerOuter`, data = londonLSOAProfiles, na.action=na.exclude) #---DEMAND SIDE MODEL modelReviews &lt;- lm(log(`airbnb_no_reviews`) ~ `bame_p` + log(`young_p`) + `nonUK` + `employees` + `house_mortg` + `house_price` + log(`culture_freq`) + `culture_rating_good` + `InnerOuter`, data = londonLSOAProfiles, na.action=na.exclude) #---AIRBNB PRICE MODEL modelPrice &lt;- lm(log(`airbnb_price`) ~ `bame_p` + `nonUK` + `employees` + `house_mortg` + `house_price` + log(culture_freq) + `culture_rating_good` + `InnerOuter`, data = londonLSOAProfiles, na.action=na.exclude) print(vif(modelFreq)) ## bame_p log(young_p) nonUK education ## 4.49 2.48 4.03 2.62 ## house_mortg house_price log(culture_freq) culture_rating_good ## 2.62 1.79 2.52 1.04 ## InnerOuter ## 2.00 print(vif(modelReviews)) ## bame_p log(young_p) nonUK employees ## 4.31 2.46 3.55 2.33 ## house_mortg house_price log(culture_freq) culture_rating_good ## 3.08 1.77 2.51 1.04 ## InnerOuter ## 1.86 print(vif(modelPrice)) ## bame_p nonUK employees house_mortg ## 4.30 3.45 2.32 3.08 ## house_price log(culture_freq) culture_rating_good InnerOuter ## 1.72 1.73 1.04 1.75 Summary of the supply model summ(modelFreq) Observations 2213 (2118 missing obs. deleted) Dependent variable log(airbnb_freq) Type OLS linear regression F(9,2203) 1344.96 R² 0.85 Adj. R² 0.85 Est. S.E. t val. p (Intercept) -3.70 0.22 -16.89 0.00 bame_p -0.01 0.00 -5.07 0.00 log(young_p) 0.87 0.03 32.56 0.00 nonUK 0.01 0.00 5.31 0.00 education 0.02 0.00 13.25 0.00 house_mortg -0.03 0.00 -13.61 0.00 house_price 0.00 0.00 0.85 0.40 log(culture_freq) 0.22 0.02 10.52 0.00 culture_rating_good 0.13 0.04 3.34 0.00 InnerOuterOuter London -0.83 0.04 -19.54 0.00 Standard errors: OLS #summary(modelFreq) Plot the residual graphs for assumption test plot(modelFreq) Summary of the demand model summ(modelReviews) Observations 2213 (2118 missing obs. deleted) Dependent variable log(airbnb_no_reviews) Type OLS linear regression F(9,2203) 717.61 R² 0.75 Adj. R² 0.74 Est. S.E. t val. p (Intercept) -3.20 0.41 -7.83 0.00 bame_p -0.02 0.00 -7.10 0.00 log(young_p) 0.93 0.04 21.45 0.00 nonUK 0.03 0.00 8.32 0.00 employees 0.03 0.00 6.90 0.00 house_mortg -0.04 0.00 -10.33 0.00 house_price 0.00 0.00 2.75 0.01 log(culture_freq) 0.30 0.03 8.72 0.00 culture_rating_good 0.20 0.06 3.15 0.00 InnerOuterOuter London -1.23 0.07 -18.24 0.00 Standard errors: OLS #summary(modelReviews) Plot the residual graphs for assumption test plot(modelReviews) Summary of the price model summ(modelPrice) Observations 2213 (2118 missing obs. deleted) Dependent variable log(airbnb_price) Type OLS linear regression F(8,2204) 233.02 R² 0.46 Adj. R² 0.46 Est. S.E. t val. p (Intercept) 3.86 0.08 46.64 0.00 bame_p -0.01 0.00 -10.34 0.00 nonUK 0.01 0.00 7.38 0.00 employees 0.00 0.00 4.30 0.00 house_mortg -0.01 0.00 -6.29 0.00 house_price 0.00 0.00 9.93 0.00 log(culture_freq) 0.05 0.01 6.49 0.00 culture_rating_good 0.07 0.02 3.72 0.00 InnerOuterOuter London -0.18 0.02 -9.61 0.00 Standard errors: OLS #summary(modelPrice) Add residuals to the londonLSOAProfiles master dataframe #---write the residuals out as a column in londonLSOAProfiles dataframe londonLSOAProfiles$modelFreq_resids &lt;- residuals(modelFreq) londonLSOAProfiles$modelReviews_resids &lt;- residuals(modelReviews) londonLSOAProfiles$modelPrice_resids &lt;- residuals(modelPrice) Plot the residuals of the normal regression model. #---how to deal with residuals with NA - make use of the row names associated with the data frame provided as input to lm #---https://stackoverflow.com/questions/6882709/how-do-i-deal-with-nas-in-residuals-in-a-regression-in-r modelFreq_resids_graph &lt;- londonLSOAProfiles %&gt;% drop_na(modelFreq_resids) %&gt;% ggplot(aes(x= modelFreq_resids)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;RESIDUAL DISTRUBUTION&quot;, subtitle = &quot;Residual distrubution of the Airbnb Price model&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;, y= &quot;Count&quot;) modelReviews_resids_graph &lt;- londonLSOAProfiles %&gt;% drop_na(modelReviews_resids) %&gt;% ggplot(aes(x= modelReviews_resids)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;RESIDUAL DISTRUBUTION&quot;, subtitle = &quot;Residual distrubution of the Airbnb Price model&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;, y= &quot;Count&quot;) modelPrice_resids_graph &lt;- londonLSOAProfiles %&gt;% drop_na(modelPrice_resids) %&gt;% ggplot(aes(x= modelPrice_resids)) + geom_histogram(alpha = 0.5, fill = &quot;#5EB296&quot;, colour = &quot;#4D4D4D&quot;) + scale_y_continuous(labels = comma) + ggtitle(&quot;RESIDUAL DISTRUBUTION&quot;, subtitle = &quot;Residual distrubution of the Airbnb Price model&quot;) + labs(x= &quot;log(Airbnb Freq)&quot;, y= &quot;Count&quot;) g &lt;- grid.arrange(modelFreq_resids_graph, modelReviews_resids_graph, modelPrice_resids_graph, ncol=3) There might be some spatial autocorrealtion in our models. Run a Durbin Watson test as recommended by MacLachlan &amp; Dennett (2019) to test for this. If is it near 2 then there is no spatial autocorrelation. #nona &lt;- londonLSOAProfiles[londonLSOAProfiles$modelFreq_resids != 0] #---run durbin-watson test print(durbinWatsonTest(modelFreq$residuals)) ## [1] 1.5 print(durbinWatsonTest(modelReviews$residuals)) ## [1] 1.58 print(durbinWatsonTest(modelPrice$residuals)) ## [1] 1.73 Plot the residuals of each of the models. Colors that are close together mean that there might be some spatial autocorrelation #---plot the residuals to see for spatial autocorrelation resid_plot1 &lt;- tm_shape(londonLSOAProfiles) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(&quot;modelFreq_resids&quot;, palette = &quot;RdYlBu&quot;) resid_plot2 &lt;- tm_shape(londonLSOAProfiles) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(&quot;modelReviews_resids&quot;, palette = &quot;RdYlBu&quot;) resid_plot3 &lt;- tm_shape(londonLSOAProfiles) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(&quot;modelPrice_resids&quot;, palette = &quot;RdYlBu&quot;) resid_plots = tmap_arrange(resid_plot1, resid_plot2, resid_plot3, ncol=3) resid_plots We can also use another calculation called Moran’s I to check for spatial autocorrelation. Use the residuals of the original model to run through to the calculation. If is it 0 then there is no spatial auto correlation. #---use Moran&#39;s I to test for spatial auto correlation modelFreq_resids_noNA &lt;- londonLSOAProfiles %&gt;% drop_na(modelFreq_resids) modelReviews_resids_noNA &lt;- londonLSOAProfiles %&gt;% drop_na(modelReviews_resids) modelPrice_resids_noNA &lt;- londonLSOAProfiles %&gt;% drop_na(modelPrice_resids) #---convert our SF object into an SP object modelFreq_resids_SP &lt;- as(modelFreq_resids_noNA, &quot;Spatial&quot;) modelReviews_resids_SP &lt;- as(modelFreq_resids_noNA, &quot;Spatial&quot;) modelPrice_resids_SP &lt;- as(modelFreq_resids_noNA, &quot;Spatial&quot;) Now that we have spatial variables for each of the models, we need to do a few other steps. 1) Calulate centroids of the shapes with resdiuals. 2) Run those polygons through a spatial weights matrix. 3) Calculate the Moran’s I #---calculate centroids of all LSOAS in London coordsFreq &lt;- coordinates(modelFreq_resids_SP) coordsReviews &lt;- coordinates(modelReviews_resids_SP) coordsPrice &lt;- coordinates(modelPrice_resids_SP) #plot(coordsW) #---generate a spatial weights matrix using nearest neighbours knnFreq &lt;- knearneigh(coordsFreq, k=4) knnReviews &lt;- knearneigh(coordsReviews, k=4) knnPrice &lt;- knearneigh(coordsPrice, k=4) Freq_knn &lt;- knn2nb(knnFreq) Reviews_knn &lt;- knn2nb(knnReviews) Price_knn &lt;- knn2nb(knnPrice) #plot(LWard_knn, coordinates(coordsW), col=&quot;blue&quot;) #---create a spatial weights matrix object from these weights Freq.knn_4_weight &lt;- nb2listw(Freq_knn, style=&quot;C&quot;, zero.policy=TRUE) Reviews.knn_4_weight &lt;- nb2listw(Reviews_knn, style=&quot;C&quot;, zero.policy=TRUE) Price.knn_4_weight &lt;- nb2listw(Price_knn, style=&quot;C&quot;, zero.policy=TRUE) #---run moran&#39;s I test on the residuals print(moran.test(modelFreq_resids_SP@data$modelFreq_resids, Freq.knn_4_weight, zero.policy=T)) ## ## Moran I test under randomisation ## ## data: modelFreq_resids_SP@data$modelFreq_resids ## weights: Freq.knn_4_weight ## ## Moran I statistic standard deviate = 22, p-value &lt;0.0000000000000002 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.315132 -0.000452 0.000202 print(moran.test(modelReviews_resids_SP@data$modelReviews_resids, Reviews.knn_4_weight, zero.policy=T)) ## ## Moran I test under randomisation ## ## data: modelReviews_resids_SP@data$modelReviews_resids ## weights: Reviews.knn_4_weight ## ## Moran I statistic standard deviate = 19, p-value &lt;0.0000000000000002 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.268238 -0.000452 0.000202 print(moran.test(modelPrice_resids_SP@data$modelPrice_resids, Price.knn_4_weight, zero.policy=T)) ## ## Moran I test under randomisation ## ## data: modelPrice_resids_SP@data$modelPrice_resids ## weights: Price.knn_4_weight ## ## Moran I statistic standard deviate = 8, p-value &lt;0.0000000000000002 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.118458 -0.000452 0.000202 6.2 Spatial lag regression model To deal with any spatial autocorrelation - if shown in the the Moran’s I statistic - we have to re-run the models through a spatial lag model #---Dealing with Spatially Autocorrelated Residuals - Spatial Lag model library(spatialreg) #---SUPPLY SIDE MODEL slag_modelFreq &lt;- lagsarlm(log(`airbnb_freq`) ~ `bame_p` + log(`young_p`) + `nonUK` + `education` + `house_mortg` + `house_price` + log(culture_freq) + culture_rating_good + `InnerOuter`, data = modelFreq_resids_SP, na.action=na.exclude, nb2listw(Freq_knn, style=&quot;C&quot;), method = &quot;eigen&quot;) # #---DEMAND SIDE MODEL slag_modelReviews &lt;- lagsarlm(log(`airbnb_no_reviews`) ~ `bame_p` + log(`young_p`) + `nonUK` + `employees` + `house_mortg` + `house_price` + log(`culture_freq`) + culture_rating_good + `InnerOuter`, data = modelReviews_resids_SP, na.action=na.exclude, nb2listw(Reviews_knn, style=&quot;C&quot;), method = &quot;eigen&quot;) # #---AIRBNB PRICE MODEL slag_modelPrice &lt;- lagsarlm(log(`airbnb_price`) ~ `bame_p` + `nonUK` + `employees` + `house_mortg` + `house_price` + log(culture_freq) + culture_rating_good + `InnerOuter`, data = modelPrice_resids_SP, na.action=na.exclude, nb2listw(Price_knn, style=&quot;C&quot;), method = &quot;eigen&quot;) Summary of the spatial lag supply model summary(slag_modelFreq) ## ## Call:lagsarlm(formula = log(airbnb_freq) ~ bame_p + log(young_p) + ## nonUK + education + house_mortg + house_price + log(culture_freq) + ## culture_rating_good + InnerOuter, data = modelFreq_resids_SP, ## listw = nb2listw(Freq_knn, style = &quot;C&quot;), na.action = na.exclude, ## method = &quot;eigen&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.56718 -0.32726 0.03281 0.34933 2.10021 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value ## (Intercept) -3.864746861872 0.176169240827 -21.9377 ## bame_p -0.006577353255 0.001390846161 -4.7290 ## log(young_p) 0.655938637916 0.022634992627 28.9790 ## nonUK 0.007114575987 0.001882870903 3.7786 ## education 0.009462974429 0.001373496658 6.8897 ## house_mortg -0.010351223767 0.001852521784 -5.5876 ## house_price -0.000000066579 0.000000041486 -1.6048 ## log(culture_freq) 0.140327046315 0.016886146511 8.3102 ## culture_rating_good 0.066976165455 0.031723413885 2.1113 ## InnerOuterOuter London -0.275028944896 0.038473292820 -7.1486 ## Pr(&gt;|z|) ## (Intercept) &lt; 0.00000000000000022 ## bame_p 0.0000022559502459 ## log(young_p) &lt; 0.00000000000000022 ## nonUK 0.0001577 ## education 0.0000000000055911 ## house_mortg 0.0000000230176933 ## house_price 0.1085302 ## log(culture_freq) &lt; 0.00000000000000022 ## culture_rating_good 0.0347505 ## InnerOuterOuter London 0.0000000000008769 ## ## Rho: 0.506, LR test value: 823, p-value: &lt; 0.000000000000000222 ## Asymptotic standard error: 0.0154 ## z-value: 33, p-value: &lt; 0.000000000000000222 ## Wald statistic: 1087, p-value: &lt; 0.000000000000000222 ## ## Log likelihood: -1940 for lag model ## ML residual variance (sigma squared): 0.318, (sigma: 0.564) ## Number of observations: 2213 ## Number of parameters estimated: 12 ## AIC: 3905, (AIC for lm: 4726) ## LM test for residual autocorrelation ## test value: 5.71, p-value: 0.016878 Summary of the spatial lag demand model summary(slag_modelReviews) ## ## Call:lagsarlm(formula = log(airbnb_no_reviews) ~ bame_p + log(young_p) + ## nonUK + employees + house_mortg + house_price + log(culture_freq) + ## culture_rating_good + InnerOuter, data = modelReviews_resids_SP, ## listw = nb2listw(Reviews_knn, style = &quot;C&quot;), na.action = na.exclude, ## method = &quot;eigen&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.851473 -0.472263 0.076411 0.592502 3.225916 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value ## (Intercept) -3.8278852762823 0.3449326470713 -11.0975 ## bame_p -0.0107589245390 0.0023716648667 -4.5364 ## log(young_p) 0.6837600655456 0.0381590866272 17.9187 ## nonUK 0.0150245478000 0.0030878524955 4.8657 ## employees 0.0184256744696 0.0034087853982 5.4053 ## house_mortg -0.0181398561573 0.0034014079543 -5.3330 ## house_price -0.0000000087975 0.0000000714463 -0.1231 ## log(culture_freq) 0.1748529451653 0.0289750727252 6.0346 ## culture_rating_good 0.0910488713677 0.0546313384622 1.6666 ## InnerOuterOuter London -0.4684272673378 0.0641806010691 -7.2986 ## Pr(&gt;|z|) ## (Intercept) &lt; 0.00000000000000022 ## bame_p 0.0000057210707605 ## log(young_p) &lt; 0.00000000000000022 ## nonUK 0.0000011405541400 ## employees 0.0000000646823521 ## house_mortg 0.0000000965802298 ## house_price 0.90200 ## log(culture_freq) 0.0000000015935790 ## culture_rating_good 0.09559 ## InnerOuterOuter London 0.0000000000002909 ## ## Rho: 0.502, LR test value: 606, p-value: &lt; 0.000000000000000222 ## Asymptotic standard error: 0.0182 ## z-value: 27.6, p-value: &lt; 0.000000000000000222 ## Wald statistic: 762, p-value: &lt; 0.000000000000000222 ## ## Log likelihood: -3144 for lag model ## ML residual variance (sigma squared): 0.945, (sigma: 0.972) ## Number of observations: 2213 ## Number of parameters estimated: 12 ## AIC: 6312, (AIC for lm: 6916) ## LM test for residual autocorrelation ## test value: 53.2, p-value: 0.00000000000030742 Summary of the spatial lag price model summary(slag_modelPrice) ## ## Call:lagsarlm(formula = log(airbnb_price) ~ bame_p + nonUK + employees + ## house_mortg + house_price + log(culture_freq) + culture_rating_good + ## InnerOuter, data = modelPrice_resids_SP, listw = nb2listw(Price_knn, ## style = &quot;C&quot;), na.action = na.exclude, method = &quot;eigen&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.765769 -0.153072 0.018155 0.166949 1.833482 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value ## (Intercept) 2.607552753690 0.133168684962 19.5808 ## bame_p -0.005927522370 0.000801777452 -7.3930 ## nonUK 0.005618192809 0.000999832125 5.6191 ## employees 0.004021075483 0.001113265531 3.6120 ## house_mortg -0.004510901513 0.001110076412 -4.0636 ## house_price 0.000000169145 0.000000023453 7.2121 ## log(culture_freq) 0.036315936079 0.007909307131 4.5915 ## culture_rating_good 0.063060591400 0.017928591132 3.5173 ## InnerOuterOuter London -0.132309188204 0.018962794134 -6.9773 ## Pr(&gt;|z|) ## (Intercept) &lt; 0.00000000000000022 ## bame_p 0.0000000000001437 ## nonUK 0.0000000191914613 ## employees 0.0003039 ## house_mortg 0.0000483225380596 ## house_price 0.0000000000005509 ## log(culture_freq) 0.0000043997769053 ## culture_rating_good 0.0004359 ## InnerOuterOuter London 0.0000000000030089 ## ## Rho: 0.307, LR test value: 137, p-value: &lt; 0.000000000000000222 ## Asymptotic standard error: 0.0252 ## z-value: 12.2, p-value: &lt; 0.000000000000000222 ## Wald statistic: 149, p-value: &lt; 0.000000000000000222 ## ## Log likelihood: -638 for lag model ## ML residual variance (sigma squared): 0.102, (sigma: 0.319) ## Number of observations: 2213 ## Number of parameters estimated: 11 ## AIC: 1298, (AIC for lm: 1433) ## LM test for residual autocorrelation ## test value: 98.4, p-value: &lt; 0.000000000000000222 Take the residuals from the spatial lag model and run again to Moran’s I calculation to see if there is any spatial autocorrelation. #---add the residuals to shape file modelFreq_resids_SP@data$slag_modelFreq_resids &lt;- slag_modelFreq$residuals modelReviews_resids_SP@data$slag_modelReviews_resids &lt;- slag_modelReviews$residuals modelPrice_resids_SP@data$slag_modelPrice_resids &lt;- slag_modelPrice$residuals #---test for spatial autocorrelation print(moran.test(modelFreq_resids_SP@data$slag_modelFreq_resids, Freq.knn_4_weight)) ## ## Moran I test under randomisation ## ## data: modelFreq_resids_SP@data$slag_modelFreq_resids ## weights: Freq.knn_4_weight ## ## Moran I statistic standard deviate = -2, p-value = 1 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.025299 -0.000452 0.000202 print(moran.test(modelReviews_resids_SP@data$slag_modelReviews_resids, Reviews.knn_4_weight)) ## ## Moran I test under randomisation ## ## data: modelReviews_resids_SP@data$slag_modelReviews_resids ## weights: Reviews.knn_4_weight ## ## Moran I statistic standard deviate = -4, p-value = 1 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.064016 -0.000452 0.000202 print(moran.test(modelPrice_resids_SP@data$slag_modelPrice_resids, Price.knn_4_weight)) ## ## Moran I test under randomisation ## ## data: modelPrice_resids_SP@data$slag_modelPrice_resids ## weights: Price.knn_4_weight ## ## Moran I statistic standard deviate = -4, p-value = 1 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.053694 -0.000452 0.000202 You can use the impact() function to interpret the coefficients of the spatial lag model similar to the global model according to this rPub: http://rstudio-pubs-static.s3.amazonaws.com/5027_52298866e7924b18b54e5c9a0a21b450.html impacts(slag_modelFreq, listw = Freq.knn_4_weight) ## Impact measures (lag, exact): ## Direct Indirect Total ## bame_p -0.0070453656 -0.0062707091 -0.013316075 ## log(young_p) 0.7026120285 0.6253579847 1.327970013 ## nonUK 0.0076208145 0.0067828858 0.014403700 ## education 0.0101363135 0.0090217991 0.019158113 ## house_mortg -0.0110877663 -0.0098686372 -0.020956403 ## house_price -0.0000000713 -0.0000000635 -0.000000135 ## log(culture_freq) 0.1503120337 0.1337848296 0.284096863 ## culture_rating_good 0.0717418623 0.0638536556 0.135595518 ## InnerOuterOuter London -0.2945986617 -0.2622067626 -0.556805424 impacts(slag_modelReviews, listw = Reviews.knn_4_weight) ## Impact measures (lag, exact): ## Direct Indirect Total ## bame_p -0.01150738826 -0.01007963841 -0.0215870267 ## log(young_p) 0.73132705052 0.64058951192 1.3719165624 ## nonUK 0.01606975719 0.01407594305 0.0301457002 ## employees 0.01970748928 0.01726233281 0.0369698221 ## house_mortg -0.01940178751 -0.01699456021 -0.0363963477 ## house_price -0.00000000941 -0.00000000824 -0.0000000177 ## log(culture_freq) 0.18701690126 0.16381325621 0.3508301575 ## culture_rating_good 0.09738284803 0.08530031953 0.1826831676 ## InnerOuterOuter London -0.50101424325 -0.43885217882 -0.9398664221 6.3 Geographically Weighted Regression Models (GWR) Finally, using code from MacLachlan &amp; Dennett (2019: Chapter 9.5) we can use a GWR model to show how the coefficients of out model vary over space https://andrewmaclachlan.github.io/CASA0005repo/gwr-and-spatially-lagged-regression.html#task-3---spatial-non-stationarity-and-geographically-weighted-regression-models-gwr library(spgwr) #---calculate kernel bandwidth SUPPLY MODEL GWRbandwidth_Freq &lt;- gwr.sel(log(`airbnb_freq`) ~ `bame_p` + log(`young_p`) + `nonUK` + `education` + `house_mortg` + `house_price` + log(culture_freq) + culture_rating_good + `InnerOuter`, data = modelFreq_resids_SP, coords=coordsFreq, adapt=T) ## Adaptive q: 0.382 CV score: 1048 ## Adaptive q: 0.618 CV score: 1069 ## Adaptive q: 0.236 CV score: 1024 ## Adaptive q: 0.146 CV score: 992 ## Adaptive q: 0.0902 CV score: 948 ## Adaptive q: 0.0557 CV score: 894 ## Adaptive q: 0.0344 CV score: 837 ## Adaptive q: 0.0213 CV score: 795 ## Adaptive q: 0.0132 CV score: 776 ## Adaptive q: 0.00813 CV score: 775 ## Adaptive q: 0.00998 CV score: 773 ## Adaptive q: 0.0103 CV score: 773 ## Adaptive q: 0.0096 CV score: 773 ## Adaptive q: 0.00975 CV score: 773 ## Adaptive q: 0.0098 CV score: 773 ## Adaptive q: 0.00971 CV score: 773 ## Adaptive q: 0.00975 CV score: 773 #---calculate kernel bandwidth DEMAND SIDE MODEL GWRbandwidth_Reviews &lt;- gwr.sel(log(`airbnb_no_reviews`) ~ `bame_p` + log(`young_p`) + `nonUK` + `employees` + `house_mortg` + `house_price` + log(`culture_freq`) + culture_rating_good + `InnerOuter`, data = modelReviews_resids_SP, coords=coordsReviews, adapt=T) ## Adaptive q: 0.382 CV score: 2784 ## Adaptive q: 0.618 CV score: 2845 ## Adaptive q: 0.236 CV score: 2731 ## Adaptive q: 0.146 CV score: 2666 ## Adaptive q: 0.0902 CV score: 2568 ## Adaptive q: 0.0557 CV score: 2435 ## Adaptive q: 0.0344 CV score: 2292 ## Adaptive q: 0.0213 CV score: 2203 ## Adaptive q: 0.0132 CV score: 2178 ## Adaptive q: 0.00825 CV score: 2204 ## Adaptive q: 0.0149 CV score: 2178 ## Adaptive q: 0.014 CV score: 2177 ## Adaptive q: 0.0139 CV score: 2177 ## Adaptive q: 0.0137 CV score: 2177 ## Adaptive q: 0.0135 CV score: 2177 ## Adaptive q: 0.0138 CV score: 2177 ## Adaptive q: 0.0137 CV score: 2177 ## Adaptive q: 0.0137 CV score: 2177 #---calculate kernel bandwidth PRICE MODEL GWRbandwidth_Price &lt;- gwr.sel(log(`airbnb_price`) ~ `bame_p` + `nonUK` + `employees` + `house_mortg` + `house_price` + log(culture_freq) + culture_rating_good + `InnerOuter`, data = modelPrice_resids_SP, coords=coordsPrice, adapt=T) ## Adaptive q: 0.382 CV score: 241 ## Adaptive q: 0.618 CV score: 244 ## Adaptive q: 0.236 CV score: 239 ## Adaptive q: 0.146 CV score: 236 ## Adaptive q: 0.0902 CV score: 232 ## Adaptive q: 0.0557 CV score: 228 ## Adaptive q: 0.0344 CV score: 225 ## Adaptive q: 0.0213 CV score: 224 ## Adaptive q: 0.0183 CV score: 225 ## Adaptive q: 0.0255 CV score: 224 ## Adaptive q: 0.0253 CV score: 224 ## Adaptive q: 0.0241 CV score: 224 ## Adaptive q: 0.0248 CV score: 224 ## Adaptive q: 0.0251 CV score: 224 ## Adaptive q: 0.0254 CV score: 224 ## Adaptive q: 0.0252 CV score: 224 ## Adaptive q: 0.0253 CV score: 224 ## Adaptive q: 0.0253 CV score: 224 Run the GWR models #---GWR model for SUPPLY gwr_modelFreq = gwr(log(`airbnb_freq`) ~ `bame_p` + log(`young_p`) + `nonUK` + `education` + `house_mortg` + `house_price` + log(culture_freq) + culture_rating_good + `InnerOuter`, data = modelFreq_resids_SP, coords=coordsW, adapt=GWRbandwidth_Freq, hatmatrix=TRUE, se.fit=TRUE) #---GWR model for DEMAND gwr_modelReviews &lt;- gwr(log(`airbnb_no_reviews`) ~ `bame_p` + log(`young_p`) + `nonUK` + `employees` + `house_mortg` + `house_price` + log(`culture_freq`) + culture_rating_good + `InnerOuter`, data = modelReviews_resids_SP, coords=coordsReviews, adapt=GWRbandwidth_Reviews, hatmatrix=TRUE, se.fit=TRUE) #---GWR model for PRICE gwr_modelPrice &lt;- gwr(log(`airbnb_price`) ~ `bame_p` + `nonUK` + `employees` + `house_mortg` + `house_price` + log(culture_freq) + culture_rating_good + `InnerOuter`, data = modelPrice_resids_SP, coords=coordsPrice, adapt=GWRbandwidth_Price, hatmatrix=TRUE, se.fit=TRUE) #---print the results of the GWR models print(gwr_modelFreq) ## Call: ## gwr(formula = log(airbnb_freq) ~ bame_p + log(young_p) + nonUK + ## education + house_mortg + house_price + log(culture_freq) + ## culture_rating_good + InnerOuter, data = modelFreq_resids_SP, ## coords = coordsW, adapt = GWRbandwidth_Freq, hatmatrix = TRUE, ## se.fit = TRUE) ## Kernel function: gwr.Gauss ## Adaptive quantile: 0.00975 (about 21 of 2213 data points) ## Summary of GWR coefficient estimates at data points: ## Min. 1st Qu. Median 3rd Qu. ## X.Intercept. -7.228820395 -4.319789816 -3.356872760 -2.172254939 ## bame_p -0.056631038 -0.020388705 -0.009492280 0.000562019 ## log.young_p. 0.441952771 0.665956210 0.763927932 0.875261391 ## nonUK -0.027227673 0.012942068 0.021944621 0.032198658 ## education -0.016416092 0.004472656 0.015710952 0.027060467 ## house_mortg -0.058304472 -0.022080586 -0.012004387 -0.004038183 ## house_price -0.000004548 -0.000000137 0.000000143 0.000000524 ## log.culture_freq. -0.180120837 0.066620191 0.117374426 0.181595751 ## culture_rating_good -0.456250455 -0.052754436 0.041295964 0.158318162 ## InnerOuterOuter.London -1.939145318 -0.792031789 -0.525073058 -0.346280818 ## Max. Global ## X.Intercept. 1.364047846 -3.70 ## bame_p 0.028639223 -0.01 ## log.young_p. 1.113674260 0.87 ## nonUK 0.077050795 0.01 ## education 0.073447528 0.02 ## house_mortg 0.039463252 -0.03 ## house_price 0.000005619 0.00 ## log.culture_freq. 0.512502876 0.22 ## culture_rating_good 0.738215167 0.13 ## InnerOuterOuter.London 0.610011644 -0.83 ## Number of data points: 2213 ## Effective number of parameters (residual: 2traceS - traceS&#39;S): 517 ## Effective degrees of freedom (residual: 2traceS - traceS&#39;S): 1696 ## Sigma (residual: 2traceS - traceS&#39;S): 0.557 ## Effective number of parameters (model: traceS): 368 ## Effective degrees of freedom (model: traceS): 1845 ## Sigma (model: traceS): 0.534 ## Sigma (ML): 0.488 ## AICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 3991 ## AIC (GWR p. 96, eq. 4.22): 3472 ## Residual sum of squares: 527 ## Quasi-global R2: 0.925 print(gwr_modelReviews) ## Call: ## gwr(formula = log(airbnb_no_reviews) ~ bame_p + log(young_p) + ## nonUK + employees + house_mortg + house_price + log(culture_freq) + ## culture_rating_good + InnerOuter, data = modelReviews_resids_SP, ## coords = coordsReviews, adapt = GWRbandwidth_Reviews, hatmatrix = TRUE, ## se.fit = TRUE) ## Kernel function: gwr.Gauss ## Adaptive quantile: 0.0137 (about 30 of 2213 data points) ## Summary of GWR coefficient estimates at data points: ## Min. 1st Qu. Median ## X.Intercept. -13.1249044566 -4.2990042540 -2.1322390416 ## bame_p -0.0673920704 -0.0264361860 -0.0168454847 ## log.young_p. 0.2962644983 0.7104407578 0.7966153681 ## nonUK -0.0253076014 0.0149662604 0.0244575458 ## employees -0.0404097031 0.0080033190 0.0232935411 ## house_mortg -0.0837240785 -0.0413736361 -0.0264244002 ## house_price -0.0000017767 0.0000000184 0.0000006749 ## log.culture_freq. -0.2143739765 0.0538927270 0.1459554600 ## culture_rating_good -0.4975656656 -0.0797557088 0.0595354611 ## InnerOuterOuter.London -2.1271858449 -1.0632967199 -0.8073073773 ## 3rd Qu. Max. Global ## X.Intercept. -0.3319368859 4.3143429650 -3.20 ## bame_p -0.0038919355 0.0358206509 -0.02 ## log.young_p. 0.9170125684 1.4457155616 0.93 ## nonUK 0.0417218356 0.1332173524 0.03 ## employees 0.0411799667 0.1323364615 0.03 ## house_mortg -0.0125132418 0.0285296028 -0.04 ## house_price 0.0000015409 0.0000072197 0.00 ## log.culture_freq. 0.2496648532 0.5996471601 0.30 ## culture_rating_good 0.1972137996 1.0141863234 0.20 ## InnerOuterOuter.London -0.5516926757 0.4718993510 -1.23 ## Number of data points: 2213 ## Effective number of parameters (residual: 2traceS - traceS&#39;S): 381 ## Effective degrees of freedom (residual: 2traceS - traceS&#39;S): 1832 ## Sigma (residual: 2traceS - traceS&#39;S): 0.957 ## Effective number of parameters (model: traceS): 268 ## Effective degrees of freedom (model: traceS): 1945 ## Sigma (model: traceS): 0.929 ## Sigma (ML): 0.871 ## AICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 6281 ## AIC (GWR p. 96, eq. 4.22): 5936 ## Residual sum of squares: 1678 ## Quasi-global R2: 0.854 print(gwr_modelPrice) ## Call: ## gwr(formula = log(airbnb_price) ~ bame_p + nonUK + employees + ## house_mortg + house_price + log(culture_freq) + culture_rating_good + ## InnerOuter, data = modelPrice_resids_SP, coords = coordsPrice, ## adapt = GWRbandwidth_Price, hatmatrix = TRUE, se.fit = TRUE) ## Kernel function: gwr.Gauss ## Adaptive quantile: 0.0253 (about 55 of 2213 data points) ## Summary of GWR coefficient estimates at data points: ## Min. 1st Qu. Median 3rd Qu. ## X.Intercept. 2.500214662 3.538704781 4.069999942 4.468446100 ## bame_p -0.015424445 -0.009380794 -0.007055678 -0.004504425 ## nonUK -0.006937825 0.001506222 0.004750759 0.007936557 ## employees -0.006229052 -0.001566686 0.003712517 0.009022674 ## house_mortg -0.018564285 -0.009237530 -0.006614671 -0.003634590 ## house_price -0.000000160 0.000000160 0.000000312 0.000000516 ## log.culture_freq. -0.066927493 0.020334462 0.035065210 0.049558440 ## culture_rating_good -0.053919306 0.007444818 0.032589542 0.071154426 ## InnerOuterOuter.London -0.338525113 -0.195922175 -0.141414710 -0.084696516 ## Max. Global ## X.Intercept. 4.964758429 3.86 ## bame_p 0.003728537 -0.01 ## nonUK 0.017629376 0.01 ## employees 0.020072036 0.00 ## house_mortg 0.002520510 -0.01 ## house_price 0.000001127 0.00 ## log.culture_freq. 0.107282776 0.05 ## culture_rating_good 0.196372849 0.07 ## InnerOuterOuter.London 0.114393575 -0.18 ## Number of data points: 2213 ## Effective number of parameters (residual: 2traceS - traceS&#39;S): 191 ## Effective degrees of freedom (residual: 2traceS - traceS&#39;S): 2022 ## Sigma (residual: 2traceS - traceS&#39;S): 0.312 ## Effective number of parameters (model: traceS): 132 ## Effective degrees of freedom (model: traceS): 2081 ## Sigma (model: traceS): 0.308 ## Sigma (ML): 0.299 ## AICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 1216 ## AIC (GWR p. 96, eq. 4.22): 1064 ## Residual sum of squares: 197 ## Quasi-global R2: 0.564 Save results as a dataframe resultsFreq &lt;- as.data.frame(gwr_modelFreq$SDF) resultsReviews &lt;- as.data.frame(gwr_modelReviews$SDF) resultsPrice &lt;- as.data.frame(gwr_modelPrice$SDF) #---attach coefficients to original dataframe modelFreq_resids_SP@data$coefCultureFreq &lt;- resultsFreq$log.culture_freq. modelFreq_resids_SP@data$coefCultureRatingGood &lt;- resultsFreq$culture_rating_good modelReviews_resids_SP@data$coefCultureFreq &lt;- resultsReviews$log.culture_freq. modelReviews_resids_SP@data$coefCultureRatingGood &lt;- resultsReviews$culture_rating_good modelPrice_resids_SP@data$coefCultureFreq &lt;- resultsPrice$log.culture_freq. modelPrice_resids_SP@data$coefCultureRatingGood &lt;- resultsPrice$culture_rating_good Plot the coefficients to see variability per model modelFreq_resids_SP_inner &lt;- modelFreq_resids_SP[modelFreq_resids_SP@data$`InnerOuter` == &quot;Inner London&quot;, ] gwr_plot1 &lt;- tm_shape(modelFreq_resids_SP) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Freq&quot;, col = &quot;coefCultureFreq&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7) gwr_plot2 &lt;- tm_shape(modelFreq_resids_SP_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Freq&quot;, col = &quot;coefCultureFreq&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) gwr_plot3 &lt;- tm_shape(modelFreq_resids_SP) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Rating (Good)&quot;, col = &quot;coefCultureRatingGood&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7 ) gwr_plot4 &lt;- tm_shape(modelFreq_resids_SP_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Rating (Good)&quot;, col = &quot;coefCultureRatingGood&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) gwr_plotFreq = tmap_arrange(gwr_plot1, gwr_plot2, gwr_plot3, gwr_plot4, ncol=2) gwr_plotFreq tmap_save(gwr_plotFreq, filename = &quot;maps/regression/gwr_modelFreq_SUPPLY.png&quot;) modelReviews_resids_SP_inner &lt;- modelReviews_resids_SP[modelReviews_resids_SP@data$`InnerOuter` == &quot;Inner London&quot;, ] gwr_plot5 &lt;- tm_shape(modelReviews_resids_SP) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Freq)&quot;, col = &quot;coefCultureFreq&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7) gwr_plot6 &lt;- tm_shape(modelReviews_resids_SP_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Freq&quot;, col = &quot;coefCultureFreq&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) gwr_plot7 &lt;- tm_shape(modelReviews_resids_SP) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Rating (Good)&quot;, col = &quot;coefCultureRatingGood&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7) gwr_plot8 &lt;- tm_shape(modelReviews_resids_SP_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Rating (Good))&quot;, col = &quot;coefCultureRatingGood&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) gwr_plotReviews = tmap_arrange(gwr_plot5, gwr_plot6, gwr_plot7, gwr_plot8, ncol=2) gwr_plotReviews tmap_save(gwr_plotReviews, filename = &quot;maps/regression/gwr_modelReviews_DEMAND.png&quot;) modelPrice_resids_SP_inner &lt;- modelPrice_resids_SP[modelPrice_resids_SP@data$`InnerOuter` == &quot;Inner London&quot;, ] gwr_plot9 &lt;- tm_shape(modelPrice_resids_SP) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Freq)&quot;, col = &quot;coefCultureFreq&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7) gwr_plot10 &lt;- tm_shape(modelPrice_resids_SP_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Freq&quot;, col = &quot;coefCultureFreq&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) gwr_plot11 &lt;- tm_shape(modelPrice_resids_SP) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Rating (Good)&quot;, col = &quot;coefCultureRatingGood&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Greater London&quot;, position = c(0.1, 0.9), size=0.7) gwr_plot12 &lt;- tm_shape(modelPrice_resids_SP_inner) + tm_borders(col = &#39;dimgray&#39;, lwd = 0.1, lty = &quot;solid&quot;) + tm_polygons(title = &quot;coeficient of Culture Rating (Good)&quot;, col = &quot;coefCultureRatingGood&quot;, palette = &quot;RdBu&quot;) + tm_layout(frame=FALSE) + tm_legend(position=c(&quot;right&quot;, &quot;bottom&quot;), outside=TRUE) + tm_credits(&quot;Inner London&quot;, position = c(0, 0.9), size=0.7) gwr_plotPrice = tmap_arrange(gwr_plot9, gwr_plot10, gwr_plot11, gwr_plot12, ncol=2) gwr_plotPrice tmap_save(gwr_plotPrice, filename = &quot;maps/regression/gwr_modelPrice_PRICE.png&quot;) "],
["references.html", "References", " References "]
]
